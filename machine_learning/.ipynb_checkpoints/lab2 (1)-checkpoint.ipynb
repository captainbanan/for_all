{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nssWjlpnJ5rU"
   },
   "source": [
    "# Лабораторная работа 2. Вероятностные модели.\n",
    "\n",
    "Результат лабораторной работы − отчет. Мы предпочитаем принимать отчеты в формате ноутбуков IPython (ipynb-файл). Постарайтесь сделать ваш отчет интересным рассказом, последовательно отвечающим на вопросы из заданий. Помимо ответов на вопросы, в отчете также должен быть код, однако чем меньше кода, тем лучше всем: нам − меньше проверять, вам — проще найти ошибку или дополнить эксперимент. При проверке оценивается четкость ответов на вопросы, аккуратность отчета и кода.\n",
    "\n",
    "Мы уверены, что выполнение лабораторных работ занимает значительное время, поэтому не рекомендуем оставлять их на последний вечер перед сдачей.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "* Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи)\n",
    "* Максимально допустимая оценка за работу — 15 баллов. Также в результате выполнения заданий у вас получится решение [задачи конкурса](https://www.kaggle.com/c/competition-2-yandex-shad-spring-2021), которое после небольшой доработки принесёт ещё 5 баллов за пробитие Medium Baseline.\n",
    "* Сдавать задание после указанного срока сдачи нельзя\n",
    "* «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов и понижают карму (подробнее о плагиате см. на странице курса)\n",
    "* Если вы нашли решение какого-то из заданий в открытом источнике, необходимо прислать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник)\n",
    "* Не оцениваются задания с удалёнными формулировками\n",
    "* Не оценивается лабораторная работа целиком, если она была выложена в открытый источник\n",
    "\n",
    "Обратите внимание, что мы не ставим оценку за просто написанный код, корректная работоспособность которого не подтверждена экспериментами.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e83Dpd2Q4s3"
   },
   "source": [
    "Цель этой лабораторной работы – научиться строить вероятностные модели и оптимизировать их параметры на примере задачи оценки риска заболевания сахарным диабетом. На задачу оценки риска болезни мы посмотрим со стороны страховой компании.\n",
    "\n",
    "Если человек перестанет проходить обследования, и страховая так и не узнает, развился ли у него диабет, то и расходов, связанных с его заболеванием не будет, т.е. можно считать, что такой человек остался здоров. То же касается людей, у которых заболевание впервые обнаружат более чем через 5 лет.\n",
    "\n",
    "Чтобы рассчитать математическое ожидание затрат на лечение клиента, страховая хочет получить в качестве результата работы модели непосредственно вероятность того, что у человека, не страдающего от заболевания, оно разовьётся в течение 5 лет. Поэтому в качестве метрики качества была выбрана бинарная кросс-энтропия (она же logloss) между предсказанными вероятностями и истинными метками классов:\n",
    "\n",
    "\n",
    "$$\\text{crossentropy}(y, p) = -\\frac{1}{N}\\sum_{i=1}^N\\left[y_i\\log(p_i) + (1-y_i)\\log(1-p_i)\\right]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zxf4DgkuKRrq"
   },
   "source": [
    "**Задание 1** Препроцессинг (1 балл) \n",
    "\n",
    "- Прочитайте описание [набора данных и задачи конкурса 2](https://www.kaggle.com/c/competition-2-yandex-shad-spring-2021).\n",
    "- Загрузите обучающий набор данных (X_train.csv, y_train.csv).\n",
    "- Обратите внимание, что часть информации о клиентах неизвестна на момент заключения договора. Соответствующие признаки отсутствуют в X_test.csv.\n",
    "- Заполните пропуски в данных. Для этого могут пригодиться методы из sklearn.impute или pandas.DataFrame.fillna.\n",
    "- По желанию используйте любой препроцессинг данных, добавляйте новые признаки и т.п. ваша задача — добиться сходимости и высокого качества полученных моделей.\n",
    "- Разбейте обучающую выборку на lab_train и lab_test, которые будете использовать для оценки всех построенных моделей в лабораторной работе. При желании использовать для оценки качества кросс-валидацию необходимо проконтролировать, чтобы для всех моделей использовались одни и те же разбиения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-Hv-63eQZvv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vRRxGAVcVuu"
   },
   "source": [
    "**Задание 2** Бэйзлайн – константное предсказание (0.5 балла).\n",
    "\n",
    "Как понять, работает ли та или иная модель, если сравить метрику не с чем? Чтобы было с чем сравнивать, соберём простой бэйзлайн: предскажем всем клиентам одну и ту же вероятность заболеть в течение 5 лет. Какое значение надо предсказать, чтобы минимизировать кросс-энтропию? Оцените качество такого предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4PYo_YWcWMV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoAhwmWCQQRz"
   },
   "source": [
    "**Задание 3** Наивный байесовский классификатор (0.5 балла).\n",
    "\n",
    "Предположим, что в каждом из классе признаки независимы и имеют нормальное распределение. Тогда подобрать параметры этого распределения поможет модель [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html). Обучите эту модель. Оцените её качество.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-kO5ewdeOUQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bSiNwxJeO5u"
   },
   "source": [
    "**Задание 4** Дискриминантный анализ (1 балл).\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_001.png)\n",
    "\n",
    "Теперь избавимся от предположения условной независимости признаков относительно целевой переменной. Таким образом, ковариационные матрицы распределений классов не обязательно будут даигональны. Мы можем наложить дополнительное условие в виде [равенства ковариационных матриц](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis) всех классов или [не делать этого](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis).\n",
    "\n",
    "Попробуйте оба варианта. Какой сработал лучше и чем это можно объяснить?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykbQjaVNqEZa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWwASoNGqE3k"
   },
   "source": [
    "**Задание 5** Логистическая регрессия (1 балл).\n",
    "\n",
    "Обучите модель логистической регрессии. Убедитесь, что модель сошлась. Удалось ли получить улучшение по сравнению с предыдущими моделями? Чем это можно объяснить?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L652BU931R4D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztZejuZ-1S68"
   },
   "source": [
    "**Задание 6** GLM – обобщённые линейные модели (1 балл).\n",
    "\n",
    "Как вы знаете, логистическая регрессия является частным случаем обобщённой линейной модели $\\mu(\\mathbb E(y|X)) = Xw$, где функция связи $\\mu(u) = \\log(\\frac{u}{1-u})$, и $y|X\\sim Bernoulli$.\n",
    "\n",
    "Учитывая, что целевая переменная бинарная, изменять класс распределений $y|X$ не имеет смысла. А вот изменить функцию связи можно.\n",
    "\n",
    "Обучите обобщённые линейные модели, в качестве функции связи использовав по крайней мере 2 разные функции, отличные от logit, использующейся в логистической регрессии.\n",
    "\n",
    "Реализацию GLM можно взять в пакете [statsmodels](https://www.statsmodels.org/stable/glm).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYyM7zftAaav"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsOjHdFuAa1B"
   },
   "source": [
    "В этой части работы мы построим теперь вероятностную модель, учитывающую специфику задачи.\n",
    "\n",
    "**Задание 7** Оценка Нельсона – Аалена (1 балл).\n",
    "\n",
    "Поскольку изначально все участники исследования были здоровы, заболеть, скажем, через месяц у них очень мало шансов. Скорее всего, это значило бы ошибку при проведении анализов. А вот заболеть через несколько лет шансов уже больше.\n",
    "\n",
    "Зависимость риска заболеть в данный момент времени $t$ при условии, что до момента $t$ человек оставался здоров, называется функцией риска ([hazard function](https://en.wikipedia.org/wiki/Survival_analysis#Hazard_function_and_cumulative_hazard_function)). В других задачах она позволяет определить, люди какого возраста наиболее подвержены заболеванию, или в какой момент эпидемии риск заразиться максимален.\n",
    "\n",
    "Оцените, какова вероятность заболеть через $t$ лет после начала исследования, воспользовавшись оценками Нельсона – Аалена:\n",
    "\n",
    "$$\\hat H_{NA}(t) = \\frac{d_t}{n_t},$$\n",
    "\n",
    "Где $n_t$ – количество участников, остававшихся здоровыми и не прекративших участие в исследовании до года $t$, $d_t$ – количество участников, заболевших в год $t.$\n",
    "\n",
    "Изобразите $\\hat H_{NA}(t)$ на графике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWzhS3IOE0UE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pS7c-iScO1I"
   },
   "source": [
    "Оценка Нельсона – Аалена не персонализирована, т.е. для неё мы никак не использовали признаки пациентов.\n",
    "\n",
    "Модель пропорциональных рисков Кокса (Cox Proportional Hazard Model, CPHM) моделирует отношение риска для пациента к среднему риску по всем пациентам для данного момента времени. Поскольку это отношение рисков – положительное число, его обычно моделируют как $e^{Xw}$, где $X$ – признаки пациентов, $w$ – обучаемый вектор весов. Таким образом,\n",
    "\n",
    "$$\\hat H_{Cox}(x, t) = \\hat H_{NA}(t)e^{xw}.$$\n",
    "\n",
    "Обратите внимание, что в линейной части CPHM не используют свободный член: в некотором смысле его роль играет $\\hat H_{NA}(t)$.\n",
    "\n",
    "Вероятность заболеть ровно в момент $t$ – это произведение вероятности не заболеть до момента $t$ на вероятность заболеть в момент $t$ при условии здоровья до момента $t$. Таким образом, функция правдоподобия имеет вид\n",
    "\n",
    "$$L = \\prod_{i=1}^N \\left[(\\hat H_{NA}(T_i)e^{x_iw})^{y_i}(1 - \\hat H_{NA}(T_i)e^{x_iw})^{1-y_i}\\prod_{t=0}^{T_i-1}\\left(1-\\hat H_{NA}(t)e^{x_iw}\\right)\\right],$$\n",
    "\n",
    "где $T_i$ – момент последней записи, $i$-го пациента.\n",
    "Максимизация правдоподобия эквивалентна минимизации «минус» нормированного логарифма правдоподобия.\n",
    "\n",
    "$$\\mathcal L = -\\frac{1}{N}\\log L = -\\frac{1}{N}\\sum_{i=1}^N\\left[y_i\\left(\\log \\hat H_{NA}(T_i) + x_iw\\right) + (1-y_i)\\left(\\log (1-\\hat H_{NA}(T_i) e^{x_iw})\\right) + \\sum_{t=0}^{T_i - 1}\\log(1-\\hat H_{NA}(t)e^{x_iw})\\right]$$\n",
    "\n",
    "$$\\nabla_w\\mathcal L = \\frac{1}{N}\\sum_{i=1}^N\\left[-x_iy_i + (1-y_i)\\frac{\\hat H_{NA}(T_i)e^{x_iw}x_i}{1-\\hat H_{NA}(T_i)e^{x_iw}} + \\sum_{t=0}^{T_i-1}\\frac{\\hat H_{NA}(t)e^{x_iw}x_i}{1-\\hat H_{NA}(t)e^{x_iw}}\\right]$$\n",
    "\n",
    "Минимизировать $\\mathcal L$ можно градиентным спуском, итеративно применяя формулу\n",
    "\n",
    "$$w:= w - \\eta \\nabla_w\\mathcal L - \\eta Cw$$\n",
    "\n",
    "где $\\eta > 0$ — размер шага (learning rate), $C\\geq 0$ — коэффициент регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEoag2gNBPe5"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBj__jTgHqBF"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "**Задание 8** Cox Proportional Hazard Model (CPHM) (3 балла).\n",
    "\n",
    " Реализуйте градиентный спуск и примените его к модели CPHM.\n",
    "\n",
    "В качестве критерия останова мы предлагаем использовать следующие условия:\n",
    " - евклидова норма разности текущего и нового векторов весов стала меньше, чем 1e-4\n",
    " - ограничение на число итераций (например, 10000)\n",
    " \n",
    "Для начальной инициализации весов нужно сравнить следующие подходы:\n",
    " - нулевая начальная инициализация\n",
    " - случайная\n",
    " \n",
    "Выполните следующие пункты и прокомментируйте полученные результаты:\n",
    "- Рассмотрите как влияет размер шага на сходимость (попробуйте не менее 5-ти различных значений).\n",
    "- Рассмотрите регуляризованную модель (не менее 5-ти различных коэффициентов регуляризации), которая описана выше, а также модель без регуляризатора. Сравните, влияет ли наличие регуляризации на скорость сходимости и качество (под качеством во всех случаях подразумевается значение исходного, нерегуляризованного функционала).\n",
    "- Исследуйте качество оптимизируемого функционала в зависимости от номера итерации (при правильной реализации и подходящем размере шага он должен убывать).\n",
    "- Влияет ли выбор начальной инициализации весов на скорость и качество?\n",
    "\n",
    "В каждом пункте требуется построить необходимые графики скорости/качества и дать исчерпывающие выводы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-svUu8CIX71"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2JKTJ3TbMVX"
   },
   "source": [
    "Метод градиентного спуска может быть весьма трудозатратен в случае большого размера обучающей выборки. Поэтому обычно используют метод стохастического градиентного спуска, где на каждой итерации выбирается случайный объект из обучающей выборки и обновление весов происходит сразу по этому объекту.\n",
    "\n",
    "**Задание 9** SGD (1 балл)\n",
    "\n",
    "Реализуйте метод стохастического градиентного спуска (sgd). В этом случае вы можете выбрать наиболее удачный функционал, исходя из предыдущего пункта (с регуляризацией, без), а также схему начальной инициализации весов.\n",
    "\n",
    "Сравните рассмотренные методы (градиентный спуск и sgd) между собой с точки зрения скорости сходимости и качества.\n",
    "\n",
    "Посмотрите как влияет размер шага на сходимость (попробуйте 4-5 различных значений)\n",
    "Исследуйте качество оптимизируемого функционала в зависимости от номера итерации\n",
    "Выберите лучший размер шага.\n",
    "\n",
    "В каждом пункте сделайте исчерпывающие выводы, подкреплённые графиками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGq9ukpHbOyr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m9nbbgybPCW"
   },
   "source": [
    "Между обновлением вектора весов по всей выборке и на одном объекте есть промежуточный подход — выбирать некоторое случайное подмножество объектов и обновлять веса по нему. Такой подход называется mini-batch. Мы не будем реализовывать этот подход в данной работе, однако иногда его бывает осмысленно использовать на практике. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdOsnrm6bPjs"
   },
   "source": [
    "Один из недостатков sgd состоит в том, что он может не доходить до локального оптимального решения, а осциллировать в окрестности. \n",
    "\n",
    "![](http://sebastianruder.com/content/images/2015/12/without_momentum.gif)\n",
    "\n",
    "Для решения этой проблемы существуют методы, позволяющие устранить этот недостаток, а также ускорить сходимость. Рассмотрим некоторые из них.\n",
    "\n",
    "![](http://nghenglim.github.io/images/2015061300.png)\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ v_t = \\gamma v_{t - 1} + \\eta \\nabla_{w}{J(w)}$$\n",
    "$$ w = w - v_t$$\n",
    "\n",
    "где\n",
    "\n",
    " - $w$ — вектор параметров\n",
    " - $J$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n",
    " \n",
    "### Adagrad \n",
    "\n",
    "Одной из сложностей является выбор размера шага (*learning rate*). Основное отличие данного метода от SGD состоит в том что размер шага определяется для каждого параметра индивидуально. Этот метод хорошо работает с разреженным данным большого объема. \n",
    "\n",
    "Обозначим градиент по параметру $w_i$ на итерации $t$ как $g_{t,i} = \\nabla_{w}J(w_i)$. \n",
    "\n",
    "В случае sgd обновление параметра $w_i$ будет выглядеть следующим образом:\n",
    "\n",
    "$$ w_{t+1, i} = w_{t, i} - \\eta \\cdot g_{t,i}$$\n",
    "\n",
    "А в случае Adagrad общий шаг $\\eta$ нормируется на посчитанные ранее градиенты для данного параметра:\n",
    "\n",
    "$$ w_{t+1, i} = w_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t,ii} + \\varepsilon}} \\cdot g_{t,i}$$\n",
    "\n",
    "где $G_t$ — диагональная матрица, где каждый диагональный элемент $i,i$ — сумма квадратов градиентов для $w_{i}$ до $t$-ой итерации. $\\varepsilon$ — гиперпараметр, позволяющий избежать деления на 0 (обычно выбирается около *1e-8*).\n",
    "\n",
    "Так как матрица $G_t$ диагональна, в векторном виде это будет выглядеть следующим образом (здесь $\\odot$ — матричное умножение):\n",
    "\n",
    "$$ w_{t+1} = w_{t} - \\dfrac{\\eta}{\\sqrt{G_t + \\varepsilon}} \\odot g_t $$\n",
    "\n",
    "### Adadelta\n",
    "\n",
    "Adadelta, в отличии от Adagrad, рассматривает не все предыдущие значения градиентов, а только последние $k$. Кроме того, сумма градиентов определяется как уменьшающееся среднеее всех предыдущих квадратов градиентов. Текущее среднее $E[g^2]_t$ на итерации $t$ будет вглядеть следующим образом:\n",
    "\n",
    "$$ E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g_t^2 $$\n",
    "\n",
    "здесь $\\gamma$ аналогична гиперпараметру из метода Momentum.\n",
    "\n",
    "Тогда обновление весов можно записать следующим образом:\n",
    "\n",
    "$$ w_{t+1} = w_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Перепишем это немного по-другому:\n",
    "\n",
    "$$ w_{t+1} = w_{t} + \\Delta w_t$$ \n",
    "$$\\Delta w_t = - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Аналогично среднему для градиентов определим среднее для параметров $w$:\n",
    "\n",
    "$$ E[\\Delta w^2]_t = \\gamma E[\\Delta w^2]_{t-1} + (1-\\gamma)\\Delta w^2 $$\n",
    "\n",
    "Введем обозначение $RMS[p]_t = \\sqrt{E[p]_t + \\varepsilon}$\n",
    "\n",
    "Тогда Adadelta выглядит следующим образом:\n",
    "\n",
    "$$\\Delta w_t = - \\dfrac{RMS[\\Delta w^2]}{RMS[ga^2]} g_t $$ \n",
    "$$ w_{t+1} = w_{t} + \\Delta w_t$$ \n",
    "\n",
    "\n",
    "Более подробно об этих и других способах оптимизации можно прочитать:\n",
    " - [здесь](http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms) очень хорошее описание различных способов оптимизации, в этом задании мы опираемся на терминологию из данной статьи\n",
    " - статья про [momentum](https://pdfs.semanticscholar.org/97da/c94ffd7a7ac09a4218848300cc7e98569d77.pdf)\n",
    " - оригинальная статья про [adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    " - оригинальная статья про [adadelta](http://arxiv.org/pdf/1212.5701v1.pdf)\n",
    " - википедия про [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) и [adagrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad)\n",
    " - [визуализация](http://imgur.com/a/Hqolp) разных способов оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAb2ou0FgL8K"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Cun8h8db_F9"
   },
   "source": [
    "**Задание 10** Реализуйте метод оптимизации *Momentum* (0.5 балла) и \n",
    "\n",
    "**Задание 11** Реализуйте один из *Adagrad*/*Adadelta* (1 балл).\n",
    "\n",
    "В заданиях 10, 11:\n",
    "- Сравните оба метода с классическим sgd с точки зрения скорости сходимости.\n",
    "- Посмотрите как значение гиперпараметра $\\gamma$ влияет на скорость сходимости и качество в методе *Momentum*.\n",
    "\n",
    "Постройте графики и опишите полученные результаты.\n",
    "\n",
    "Дало ли преимущество использование адаптивного шага в методе *Adagrad*/*Adadelta*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fz8ZzohGhOUm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oh172idwhSBU"
   },
   "source": [
    "**Задание 12** Предсказание с помощью CPHM, интерфейс sklearn (1 балл)\n",
    "\n",
    "\n",
    "Чтобы сравнить CPHM с предыдущими моделями, надо научиться предсказывать целевую переменную. С помощью значений персональной функции риска $\\hat H_{Cox}(x, t)$ можно выразить вероятность того, что человек в течение 5 лет не заболеет при условии, что он не прекратит участие в эксперементе по другим причинам. Вероятность того, что человек не прекратит участие в эксперименте, оставаясь здоровым, также можно оценить из данных.\n",
    "\n",
    "Реализуйте модель CPHM, унаследовав класс модели от [BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator). Модель должна поддерживать методы fit, predict, predict_proba. Это вам позволит в следующем задании её откалибровать. При тестировании модели следует передать то время, через которое вас интересует состояние пациента (в случае конкурса, 5 лет).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvYwCN55gO8j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xtdNig2NDoQ"
   },
   "source": [
    "\n",
    "**Задание 13** Калибровка классификатора (2 балла)\n",
    "\n",
    "Сравнительно простой способ уточнить оценки вероятности и уменьшить logloss -- откалибровать модель. Вообще говоря, сравнивать между собой некалиброванные модели по logloss некорректно: хорошая модель с нарушенной калибровкой может иметь сколь угодно большое значение logloss.\n",
    "\n",
    "Идея калибровки состоит в том, чтобы подобрать простое преобразование, которое превратит выходы моделей в вероятности принадлежности классам.\n",
    "\n",
    "Есть несколько известных методов калибровки:\n",
    " - Калибровка Платта.\n",
    " - Изотоническая регрессия.\n",
    "\n",
    "Обратите внимание, что преобразование необходимо обучать на отложенной выборке (то есть классификатор и калибровка должны обучаться на разных подмножествах данных), иначе можно переобучиться. Калибровку можно применять к любым классификаторам (где это разумно и необходимо), особенно к тем, которые не оптимизируют logloss явно.\n",
    "\n",
    "Для калибровки классификатора в sklearn возможны два подхода:\n",
    " - взять уже обученный классификатор и откалибровать его на отложенной выборке\n",
    " - откалибровать по кросс-валидации: калибровочному классификатору передается вся обучающая выборка, которая внутри разбивается на обучающую и калибровочную, после чего происходит усреднение вероятностей по фолдам.\n",
    " \n",
    "Подробнее об этом можно прочитать в [документации](http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV). Также [здесь](https://jmetzen.github.io/2015-04-14/calibration.html) можно узнать подробности о калибровке в sklearn от автора.\n",
    "\n",
    "Используйте оба описанных выше подхода (калибровку Платта и изотоническую регрессию) для калибровки моделей, построенных в лабораторной работе. Для каждой модели постройте график, на котором будут изображены [калибровочные кривые](http://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html): идеальная, исходного классификатора, а также для каждого из методов. Калибровочная кривая строится путем упорядочения всех объектов по предсказанному значению, которые разбиваются на бины. По оси OX откладывается среднее предсказанное значение вероятности по бину, а по OY — доля положительных примеров. В случае идеальных вероятностей это будет прямая.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jnNocIJofF5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYR8QBLooeWY"
   },
   "source": [
    "Выполнив задания, не забудьте отправить решение в [конкурс](https://www.kaggle.com/c/competition-2-yandex-shad-spring-2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qv8F9EcNNEVg"
   },
   "source": [
    "**Выводы** (0.5 балла)\n",
    "\n",
    "- Какие есть достоинства и недостатки у рассмотренных в лабораторной работе моделей?\n",
    "- Какие модели сильно улучшили свой logloss после калибровки, а какие - нет? Почему так произошло?\n",
    "- Какие из рассмотренных методов могут обучиться на маленьких датасетах? А какие применимы для очень больших датасетов? Какие проблемы могут возникнуть при применении к большим датасетам и как их можно решить?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wUMGH8PnHLG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab2_2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
