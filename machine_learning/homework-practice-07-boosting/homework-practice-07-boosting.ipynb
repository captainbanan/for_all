{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "# Практическое задание 7. Бустинговое\n",
    "\n",
    "## Общая информация\n",
    "\n",
    "Дата выдачи: 06.12.2021\n",
    "\n",
    "Мягкий дедлайн: 18.12.2021 23:59 MSK\n",
    "\n",
    "Жёсткий дедлайн: 19.12.2021 23:59 MSK\n",
    "\n",
    "## Оценивание и штрафы\n",
    "\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "## Формат сдачи\n",
    "Задания сдаются через систему anytask. Посылка должна содержать:\n",
    "* Ноутбук homework-practice-07-Username.ipynb\n",
    "\n",
    "Username — ваша фамилия на латинице"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## О задании\n",
    "\n",
    "В этом задании вам предстоит вручную запрограммировать один из самых мощных алгоритмов машинного обучения — бустинг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tqdm.notebook import tqdm\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = load_npz('x.npz').toarray()\n",
    "y = np.load('y.npy')\n",
    "y = y / 2 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23532,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим на обучающую, валидационную и тестовую выборки (`random_state` оставьте равным 1337 для воспроизводимости)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18825, 169), (2354, 169), (2353, 169))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1337)\n",
    "\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size=0.5, random_state=1337)\n",
    "\n",
    "x_train.shape, x_valid.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Реализация градиентного бустинга (4 балла)\n",
    "\n",
    "Вам нужно дописать код в файлике `boosting.py`. Для вас уже подготовлен шаблон класса `Boosting`, вы можете менять его по своему усмотрению.\n",
    "\n",
    "### Инструкции для функций:\n",
    "\n",
    "#### `__init__`\n",
    "\n",
    "В `__init__` приходит кучка параметров, распишем что есть что:\n",
    "\n",
    " - `base_model_class` - класс базовой модели нашего бустинга\n",
    " - `base_model_params` - словарь с гиперпараметрами для базовой модели\n",
    " - `n_estimators` - какое количество базовых моделей нужно обучить\n",
    " - `learning_rate` - темп обучения, должен быть из полуинтервала $(0, 1]$\n",
    " - `subsample` - доля объектов, на которой будет обучаться базовая модель (какую часть составляет бутстрапная выборка от исходной обучающей)\n",
    " - `early_stopping_rounds` - число итераций, после которых при отсутствии улучшения качества на валидационной выборке обучение останавливается\n",
    " - `plot` - строить ли после обучения всех базовых моделей график с качеством\n",
    "\n",
    "#### `fit`\n",
    "\n",
    "В `fit` приходит две выборки, обучающая и валидационная. На обучающей мы обучаем новые базовые модели, на валидационной считаем качество для ранней остановки (если это предусматривают параметры).\n",
    "\n",
    "Сначала нам нужно сделать какую-то нулевую модель, сделать предсказания для обучающей и валидационной выборок (в шаблоне это нулевая модель, соответственно предсказания это просто `np.zeros`). После этого нужно обучить `n_estimators` базовых моделей (как и на что обучаются базовые модели смотрите в лекциях и семинарах). После каждой обученной базовой модели мы должны обновить текущие предсказания, посчитать ошибку на обучающей и валидационной выборках (используем `loss_fn` для этого), проверить на раннюю остановку.\n",
    "\n",
    "После всего цикла обучения надо нарисовать график (если `plot`).\n",
    "\n",
    "\n",
    "#### `fit_new_base_model`\n",
    "\n",
    "В `fit_new_base_model` приходит обучающая выборка (целиком) и текущие предсказания для неё. Мы должны сгенерировать бутстрап выборку для обучения базовой модели и обучить базовую модель. После обучения модели запускаем поиск оптимальной гаммы, добавляем новую модель и гамму (не забываем про темп обучения) в соответствующие списки.\n",
    "\n",
    "#### `predict_proba`\n",
    "\n",
    "В `predict_proba` приходит выборка, нужно предсказать вероятности для неё. Суммируем предсказания базовых моделей на этой выборке (не забываем про гаммы) и накидываем сигмоиду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from boosting import Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка кода\n",
    "\n",
    "У автора задания всё учится около одной секунды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 0 ns, total: 1.06 s\n",
      "Wall time: 1.05 s\n",
      "Train ROC-AUC 0.9881\n",
      "Valid ROC-AUC 0.9118\n",
      "Test ROC-AUC 0.9179\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boosting = Boosting(DecisionTreeRegressor, n_estimators=10, plot=True)\n",
    "\n",
    "%time boosting.fit(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "assert len(boosting.models) == boosting.n_estimators\n",
    "assert len(boosting.gammas) == boosting.n_estimators\n",
    "\n",
    "assert boosting.predict_proba(x_test).shape == (x_test.shape[0], 2)\n",
    "\n",
    "print(f'Train ROC-AUC {boosting.score(x_train, y_train):.4f}')\n",
    "print(f'Valid ROC-AUC {boosting.score(x_valid, y_valid):.4f}')\n",
    "print(f'Test ROC-AUC {boosting.score(x_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Обучение градиентного бустинга (1 балл)\n",
    "\n",
    "Оцените качество на тестовой выборке вашей имплементации бустинга для различной максимальной глубины решающего дерева в качестве базовой модели. Здесь и далее мы будем использовать метрику ROC-AUC.\n",
    "\n",
    "Перебирайте максимальную глубину от 1 до 30 с шагом 2 (остальные параметры бустинга стоит оставить равными по умолчанию). Постройте график зависимости качества на обучающей и тестовой выборке в зависимости от глубины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth is 1\n",
      "Train ROC-AUC 0.8497\n",
      "Test ROC-AUC 0.8349\n",
      "\n",
      "Depth is 3\n",
      "Train ROC-AUC 0.9386\n",
      "Test ROC-AUC 0.9176\n",
      "\n",
      "Depth is 5\n",
      "Train ROC-AUC 0.9543\n",
      "Test ROC-AUC 0.9279\n",
      "\n",
      "Depth is 7\n",
      "Train ROC-AUC 0.9656\n",
      "Test ROC-AUC 0.9307\n",
      "\n",
      "Depth is 9\n",
      "Train ROC-AUC 0.9739\n",
      "Test ROC-AUC 0.9338\n",
      "\n",
      "Depth is 11\n",
      "Train ROC-AUC 0.9790\n",
      "Test ROC-AUC 0.9279\n",
      "\n",
      "Depth is 13\n",
      "Train ROC-AUC 0.9826\n",
      "Test ROC-AUC 0.9308\n",
      "\n",
      "Depth is 15\n",
      "Train ROC-AUC 0.9844\n",
      "Test ROC-AUC 0.9291\n",
      "\n",
      "Depth is 17\n",
      "Train ROC-AUC 0.9860\n",
      "Test ROC-AUC 0.9258\n",
      "\n",
      "Depth is 19\n",
      "Train ROC-AUC 0.9873\n",
      "Test ROC-AUC 0.9228\n",
      "\n",
      "Depth is 21\n",
      "Train ROC-AUC 0.9880\n",
      "Test ROC-AUC 0.9172\n",
      "\n",
      "Depth is 23\n",
      "Train ROC-AUC 0.9882\n",
      "Test ROC-AUC 0.9190\n",
      "\n",
      "Depth is 25\n",
      "Train ROC-AUC 0.9881\n",
      "Test ROC-AUC 0.9195\n",
      "\n",
      "Depth is 27\n",
      "Train ROC-AUC 0.9891\n",
      "Test ROC-AUC 0.9155\n",
      "\n",
      "Depth is 29\n",
      "Train ROC-AUC 0.9889\n",
      "Test ROC-AUC 0.9136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "depths = range(1, 30, 2)\n",
    "\n",
    "for depth in depths:\n",
    "    boosting = Boosting(DecisionTreeRegressor, base_model_params={'max_depth': depth}, n_estimators=10)\n",
    "    boosting.fit(x_train, y_train, x_valid, y_valid)\n",
    "    print(f'Depth is {depth}')\n",
    "    print(f'Train ROC-AUC {boosting.score(x_train, y_train):.4f}')\n",
    "    print(f'Test ROC-AUC {boosting.score(x_test, y_test):.4f}\\n')\n",
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Какая из моделей имеет лучшее качество? Как вы можете это объяснить?**\n",
    "\n",
    "`### ваше решение тут ###`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшее качество на глубине 9, по трейну видно, что с глубиной побольше он просто недооубучается. Потом тест начинает стагнировать, а трейн расти, это он уже подстраивается под трейн, а потом уже и тест начинает падать, это он уже сильно переобучлися"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Подбираем гиперпараметры и ищем лучшую модель (2 балла)\n",
    "\n",
    "Подберите по валидационной выборке основные гиперпараметры для вашей модели бустинга. Следует подобрать все основные параметры для самого градиентного бустинга и для самих базовых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7954bef82f4a4b89950ccb0b4afbb1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7a5da88bb54bfb8ae05a92d115e2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94588d9658f44c009868b5af197d8766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4146ef131d41ec8eaba79dc5530e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4675/3992780112.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 }, subsample=subsample, early_stopping_rounds=early_stopping_rounds, \n\u001b[1;32m     18\u001b[0m                                                     learning_rate=learning_rate, n_estimators=n_estimators)\n\u001b[0;32m---> 19\u001b[0;31m                                 \u001b[0mboosting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mboosting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                                     best_params = dict(base_model_params={\n",
      "\u001b[0;32m~/for_all/machine_learning/homework-practice-07-boosting/boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, x_valid, y_valid)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsample\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_new_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtrain_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/for_all/machine_learning/homework-practice-07-boosting/boosting.py\u001b[0m in \u001b[0;36mfit_new_base_model\u001b[0;34m(self, x, y, predictions)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_new_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgammas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_optimal_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/for_all/machine_learning/homework-practice-07-boosting/boosting.py\u001b[0m in \u001b[0;36mfind_optimal_gamma\u001b[0;34m(self, y, old_predictions, new_predictions)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_optimal_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_predictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mgammas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_predictions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnew_predictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/for_all/machine_learning/homework-practice-07-boosting/boosting.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_optimal_gamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_predictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mgammas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_predictions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnew_predictions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/for_all/machine_learning/homework-practice-07-boosting/boosting.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y, z)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_derivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_derivative2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_score = -1\n",
    "best_params = []\n",
    "\n",
    "for max_depth in tqdm(range(1, 30, 7)):\n",
    "    for min_samples_split in tqdm(range(2, 20, 6)):\n",
    "        for min_samples_leaf in tqdm(range(1, 10, 4)):\n",
    "            for max_features in ['sqrt', 'log2']:\n",
    "                for subsample in tqdm(np.arange(0.1, 0.9, 0.2)):\n",
    "                    for early_stopping_rounds in range(5, 20, 7):\n",
    "                        for learning_rate in np.arange(0.01, 0.1, 0.03):\n",
    "                            for n_estimators in range(5, 20, 6):\n",
    "                                boosting = Boosting(DecisionTreeRegressor, base_model_params={\n",
    "                                    'max_depth': depth,\n",
    "                                    'min_samples_split': min_samples_split,\n",
    "                                    'min_samples_leaf': min_samples_leaf,\n",
    "                                    'max_features': max_features\n",
    "                                }, subsample=subsample, early_stopping_rounds=early_stopping_rounds, \n",
    "                                                    learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "                                boosting.fit(x_train, y_train, x_valid, y_valid)\n",
    "                                if boosting.score(x_valid, y_valid) > best_score:\n",
    "                                    best_params = dict(base_model_params={\n",
    "                                    'max_depth': depth,\n",
    "                                    'min_samples_split': min_samples_split,\n",
    "                                    'min_samples_leaf': min_samples_leaf,\n",
    "                                    'max_features': max_features\n",
    "                                }, subsample=subsample, early_stopping_rounds=early_stopping_rounds, \n",
    "                                                    learning_rate=learning_rate, n_estimators=n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'base_model_params': {'max_depth': 29,\n",
       "   'min_samples_split': 2,\n",
       "   'min_samples_leaf': 1,\n",
       "   'max_features': 'sqrt'},\n",
       "  'subsample': 0.7000000000000001,\n",
       "  'early_stopping_rounds': 5,\n",
       "  'learning_rate': 0.06999999999999999,\n",
       "  'n_estimators': 11},\n",
       " -1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "примерно час работает, я разок посчитал, а потом все скинул случайно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Пробуем разные библиотеки (2 балла)\n",
    "\n",
    "Выберите себе библиотеку с реализацией градиентного бустинга по вкусу из следующих трёх - [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_intro.html), [LightGBM](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html), [CatBoost](https://catboost.ai/en/docs/concepts/python-quickstart). Подберите основные гиперпараметры, постарайтесь добиться наилучшего качества на тестовых данных. Сравните результаты своей реализации с результатами библиотечного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[05:10:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:10:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           enable_categorical=False, gamma=None,\n",
       "                                           gpu_id=None, importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=0.02,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None...\n",
       "                                           predictor=None, random_state=None,\n",
       "                                           reg_alpha=None, reg_lambda=None,\n",
       "                                           scale_pos_weight=None, silent=True,\n",
       "                                           subsample=None, tree_method=None,\n",
       "                                           validate_parameters=None,\n",
       "                                           verbosity=None),\n",
       "                   n_jobs=8,\n",
       "                   param_distributions={'colsample_bytree': [0.6, 0.8, 1.0],\n",
       "                                        'gamma': [0.5, 1, 1.5, 2, 5],\n",
       "                                        'max_depth': [3, 4, 5],\n",
       "                                        'min_child_weight': [1, 5, 10],\n",
       "                                        'subsample': [0.6, 0.8, 1.0]},\n",
       "                   scoring='roc_auc', verbose=4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=70, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)\n",
    "grid = RandomizedSearchCV(estimator=xgb, param_distributions=params, scoring='roc_auc', n_jobs=8, verbose=4 )\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9327614493570536"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:09:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END colsample_bytree=1.0, gamma=2, max_depth=3, min_child_weight=5, subsample=0.6;, score=0.953 total time=   4.3s\n",
      "[05:09:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.5, max_depth=5, min_child_weight=1, subsample=0.6;, score=0.967 total time=   4.5s\n",
      "[05:09:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END colsample_bytree=0.8, gamma=0.5, max_depth=4, min_child_weight=10, subsample=0.6;, score=0.952 total time=   4.6s\n",
      "[05:09:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END colsample_bytree=1.0, gamma=1, max_depth=4, min_child_weight=5, subsample=0.8;, score=0.951 total time=   4.8s\n",
      "[05:09:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END colsample_bytree=1.0, gamma=1, max_depth=5, min_child_weight=10, subsample=0.6;, score=0.952 total time=   7.0s\n",
      "[05:09:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END colsample_bytree=0.8, gamma=2, max_depth=4, min_child_weight=5, subsample=0.6;, score=0.955 total time=   4.8s\n",
      "[05:09:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=1, max_depth=4, min_child_weight=10, subsample=0.8;, score=0.960 total time=   3.2s\n",
      "[05:09:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.5, max_depth=5, min_child_weight=5, subsample=1.0;, score=0.958 total time=   5.8s\n",
      "[05:09:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.5, max_depth=5, min_child_weight=1, subsample=0.6;, score=0.966 total time=   6.8s\n",
      "[05:09:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END colsample_bytree=0.8, gamma=0.5, max_depth=4, min_child_weight=5, subsample=0.8;, score=0.957 total time=   6.4s\n",
      "[05:09:46] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END colsample_bytree=1.0, gamma=1, max_depth=5, min_child_weight=10, subsample=0.6;, score=0.959 total time=  10.0s\n",
      "[05:09:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=1, max_depth=4, min_child_weight=10, subsample=0.8;, score=0.954 total time=   3.3s\n",
      "[05:09:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END colsample_bytree=1.0, gamma=2, max_depth=3, min_child_weight=5, subsample=0.6;, score=0.948 total time=   4.3s\n",
      "[05:09:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.5, max_depth=5, min_child_weight=1, subsample=0.6;, score=0.962 total time=   4.5s\n",
      "[05:09:36] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END colsample_bytree=0.8, gamma=0.5, max_depth=4, min_child_weight=10, subsample=0.6;, score=0.957 total time=   4.5s\n",
      "[05:09:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END colsample_bytree=0.8, gamma=0.5, max_depth=4, min_child_weight=5, subsample=0.8;, score=0.953 total time=   3.9s\n",
      "[05:09:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END colsample_bytree=1.0, gamma=1, max_depth=5, min_child_weight=10, subsample=0.6;, score=0.962 total time=   7.0s\n",
      "[05:09:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END colsample_bytree=1.0, gamma=1.5, max_depth=3, min_child_weight=5, subsample=1.0;, score=0.942 total time=   3.2s\n",
      "[05:09:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END colsample_bytree=0.8, gamma=2, max_depth=4, min_child_weight=5, subsample=0.6;, score=0.953 total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:09:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=0.5, max_depth=5, min_child_weight=5, subsample=1.0;, score=0.965 total time=   5.7s\n",
      "[05:09:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END colsample_bytree=0.6, gamma=0.5, max_depth=5, min_child_weight=1, subsample=0.6;, score=0.959 total time=   6.8s\n",
      "[05:09:40] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END colsample_bytree=0.8, gamma=0.5, max_depth=4, min_child_weight=5, subsample=0.8;, score=0.952 total time=   4.3s\n",
      "[05:09:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:44] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END colsample_bytree=1.0, gamma=1, max_depth=5, min_child_weight=10, subsample=0.6;, score=0.956 total time=   7.0s\n",
      "[05:09:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END colsample_bytree=1.0, gamma=1.5, max_depth=3, min_child_weight=5, subsample=1.0;, score=0.946 total time=   3.2s\n",
      "[05:09:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END colsample_bytree=0.8, gamma=2, max_depth=4, min_child_weight=5, subsample=0.6;, score=0.958 total time=   6.2s\n",
      "[05:09:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=0.5, max_depth=5, min_child_weight=5, subsample=1.0;, score=0.961 total time=   3.2s\n",
      "[05:09:30] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END colsample_bytree=0.6, gamma=0.5, max_depth=5, min_child_weight=5, subsample=1.0;, score=0.963 total time=   3.2s\n",
      "[05:09:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.5, max_depth=5, min_child_weight=1, subsample=0.6;, score=0.960 total time=   4.4s\n",
      "[05:09:38] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END colsample_bytree=0.8, gamma=0.5, max_depth=4, min_child_weight=5, subsample=0.8;, score=0.955 total time=   4.5s\n",
      "[05:09:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5] END colsample_bytree=1.0, gamma=1, max_depth=4, min_child_weight=5, subsample=0.8;, score=0.954 total time=   7.7s\n",
      "[05:09:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END colsample_bytree=1.0, gamma=1.5, max_depth=3, min_child_weight=5, subsample=1.0;, score=0.941 total time=   5.4s\n",
      "[05:09:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END colsample_bytree=0.6, gamma=1, max_depth=4, min_child_weight=10, subsample=0.8;, score=0.962 total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:09:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END colsample_bytree=1.0, gamma=2, max_depth=3, min_child_weight=5, subsample=0.6;, score=0.945 total time=   4.3s\n",
      "[05:09:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=0.5, max_depth=5, min_child_weight=5, subsample=1.0;, score=0.958 total time=   3.2s\n",
      "[05:09:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END colsample_bytree=0.8, gamma=0.5, max_depth=4, min_child_weight=10, subsample=0.6;, score=0.952 total time=   4.5s\n",
      "[05:09:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END colsample_bytree=0.8, gamma=0.5, max_depth=4, min_child_weight=5, subsample=0.8;, score=0.961 total time=   3.9s\n",
      "[05:09:43] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END colsample_bytree=1.0, gamma=1, max_depth=4, min_child_weight=5, subsample=0.8;, score=0.949 total time=   4.7s\n",
      "[05:09:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END colsample_bytree=1.0, gamma=1, max_depth=5, min_child_weight=10, subsample=0.6;, score=0.952 total time=   7.0s\n",
      "[05:09:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END colsample_bytree=0.6, gamma=1, max_depth=4, min_child_weight=10, subsample=0.8;, score=0.957 total time=   3.3s\n",
      "[05:09:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END colsample_bytree=0.6, gamma=1, max_depth=4, min_child_weight=10, subsample=0.8;, score=0.954 total time=   3.1s\n",
      "[05:09:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END colsample_bytree=1.0, gamma=2, max_depth=3, min_child_weight=5, subsample=0.6;, score=0.942 total time=   6.5s\n",
      "[05:09:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END colsample_bytree=0.8, gamma=0.5, max_depth=4, min_child_weight=10, subsample=0.6;, score=0.955 total time=   7.0s\n",
      "[05:09:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END colsample_bytree=1.0, gamma=1, max_depth=4, min_child_weight=5, subsample=0.8;, score=0.948 total time=   7.7s\n",
      "[05:09:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END colsample_bytree=1.0, gamma=1.5, max_depth=3, min_child_weight=5, subsample=1.0;, score=0.952 total time=   5.6s\n",
      "[05:09:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5] END colsample_bytree=0.8, gamma=2, max_depth=4, min_child_weight=5, subsample=0.6;, score=0.952 total time=   6.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:09:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5] END colsample_bytree=1.0, gamma=2, max_depth=3, min_child_weight=5, subsample=0.6;, score=0.943 total time=   6.5s\n",
      "[05:09:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END colsample_bytree=0.8, gamma=0.5, max_depth=4, min_child_weight=10, subsample=0.6;, score=0.960 total time=   7.0s\n",
      "[05:09:41] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END colsample_bytree=1.0, gamma=1, max_depth=4, min_child_weight=5, subsample=0.8;, score=0.958 total time=   7.7s\n",
      "[05:09:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5] END colsample_bytree=1.0, gamma=1.5, max_depth=3, min_child_weight=5, subsample=1.0;, score=0.943 total time=   5.3s\n",
      "[05:09:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[05:09:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5] END colsample_bytree=0.8, gamma=2, max_depth=4, min_child_weight=5, subsample=0.6;, score=0.961 total time=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/captainbanana/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "roc_auc_score(y_test, grid.predict_proba(x_test)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (бонус). Пробуем ещё немножко библиотек (1 балл)\n",
    "\n",
    "Существуют библиотеки для подбора гиперпараметров, попробуйте использовать какую-нибудь из следующих двух - [Hyperopt](https://github.com/hyperopt/hyperopt), [Optuna](https://optuna.org/). Сравните полученное качество с вашим ручным перебором. Используйте эту библиотеку чтобы подобрать параметры и для своей реалзиации и для библиотечной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Интерпретация бустингового (1 балл)\n",
    "\n",
    "Постройте калибровочную кривую для вашей лучшей модели бустинга (из тех, что используют вашу реализацию).\n",
    "Насколько хорошо бустинг оценивает вероятности? Постройте также калибровочную кривую для логистической регресии, сравните их между собой. Проанализируйте полученные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем оценить важность признаков для бустинга.\n",
    "\n",
    "Поскольку наша базовая модель - это дерево из `sklearn`, мы можем вычислить важность признака отдельно для каждого дерева и усреднить (воспользуйтесь `feature_importances_` у `DecisionTreeRegressor`), после этого нормировать значения, чтобы они суммировались в единицу (обратите внимание, что они должны быть неотрицательными - иначе вы что-то сделали не так).\n",
    "\n",
    "Допишите в вашей реализации бустинга функцию `feature_importances_` чтобы она возвращала описанные выше важности признаков.\n",
    "\n",
    "Нарисуйте столбчатую диаграмму важности признаков. На соседнем графике нарисуйте важность признаков для логистической регрессии, для этого используйте модули весов. Сравните графики. Проанализируйте полученные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, чаще всего излишние признаки могут вредить качеству бустинга. Попробуйте отфильтровать на основании диаграммы хвост наименее важных признаков и снова обучить модель (с теми же гиперпараметрами). Стало ли лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7 (бонус). Блендинговое (1 балл)\n",
    "\n",
    "Реализуйте блендинг над вашими лучшими моделями (ваша реализация с лучшими гиперпараметрами + все модели из библиотек с лучшими гиперпараметрами). Улучшилось ли качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Социализационный бонус. Новогоднее 🎆 (0.5 балла)\n",
    "\n",
    "Сфотографируйтесь с наряженной новогодней или рождественской ёлкой! Приложите фотографию, опишите свои впечатления, чего вы ждете от нового 2022 года?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.ibb.co/p2XCKwN/photo-2021-12-19-13-16-23.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.ibb.co/p2XCKwN/photo-2021-12-19-13-16-23.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "экзамены там сдать и все такое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_3",
   "language": "python",
   "name": "all_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "492px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
